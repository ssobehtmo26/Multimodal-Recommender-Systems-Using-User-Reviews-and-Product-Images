{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JqByBCJT8Al"
      },
      "source": [
        "# Multimodal Recommender Systems Using User Reviews and Product Images\n",
        "## By: Om Choudhary\n",
        "\n",
        "### Motivation\n",
        "\n",
        "In an era where online shopping dominates, building recommender systems that go beyond simple numeric ratings or keyword matching has become essential. I chose this topic to explore how integrating **user-generated reviews** with **product images** can enhance rating prediction, which is foundational to effective personalized recommendations. This is particularly useful for fashion e-commerce, where visual cues are as important as textual sentiment.\n",
        "\n",
        "### Historical Context: Multimodal Learning in Recommendation\n",
        "\n",
        "Recommender systems have evolved from:\n",
        "\n",
        "- **Collaborative filtering** (Matrix Factorization)\n",
        "\n",
        "- To **content-based filtering** (text reviews, metadata)\n",
        "\n",
        "- And now to **multimodal learning**, where multiple data types (e.g., text, image, audio) are combined.\n",
        "\n",
        "Recent research (e.g., Chen et al., 2020; McAuley et al., 2015) has shown that fusing reviews and product images significantly improves predictive performance, especially in the cold-start scenario. Multimodal learning allows systems to learn richer product representations, capturing both style and sentiment.\n",
        "\n",
        "### Summary of Our Workflow\n",
        "\n",
        "####Step 1: Data Loading & Cleaning\n",
        "\n",
        "- Downloaded and loaded Amazon_Fashion.jsonl (reviews) and meta_Amazon_Fashion.jsonl (product metadata).\n",
        "\n",
        "- Sampled 25,000 records for efficiency.\n",
        "\n",
        "- Merged on parent_asin.\n",
        "\n",
        "- Extracted the first available image URL (hi-res > large > thumb).\n",
        "\n",
        "####Step 2: Image Download\n",
        "\n",
        "- Downloaded images to local disk using requests and PIL.\n",
        "\n",
        "- Limited to 2,000 images to conserve resources.\n",
        "\n",
        "####Step 3: Feature Extraction\n",
        "\n",
        "- **Text**: Used **DistilBERT** to get 768-dimensional contextual embeddings.\n",
        "\n",
        "- **Image**: Used **ResNet50** to extract 2048-dimensional visual embeddings.\n",
        "\n",
        "- Concatenated both for a 2816-dimensional multimodal representation.\n",
        "\n",
        "####Step 4: Model Training\n",
        "\n",
        "- Used a **deep neural network classifier** with:\n",
        "\n",
        "  - 3 hidden layers (1024, 512, 128)\n",
        "\n",
        "  - BatchNorm + Dropout for regularization\n",
        "\n",
        "  - `Adam` optimizer with learning rate scheduling\n",
        "\n",
        "- Handled **class imbalance** with `class_weight`\n",
        "\n",
        "####Step 5: Evaluation\n",
        "\n",
        "- Metrics: Accuracy, Confusion Matrix, Classification Report\n",
        "\n",
        "- Final Accuracy: ~**60**%\n",
        "\n",
        "##  Key Learning Outcomes\n",
        "\n",
        "- **Multimodal fusion** significantly improves performance compared to using text or image alone.\n",
        "- **BERT-style embeddings** capture sentiment better than traditional TF-IDF.\n",
        "- Image features complement text, especially for visual domains like fashion.\n",
        "\n",
        "##  Reflections\n",
        "### What Surprised Me:\n",
        "- DistilBERT was extremely effective even without fine-tuning.\n",
        "- Combining two very different modalities was relatively smooth using pretrained models.\n",
        "###  Scope for Improvement:\n",
        "- Add **user personalization** (user IDs, embeddings)\n",
        "- Use **fine-tuned BERT** or **CLIP** for joint vision-language learning\n",
        "- Scale up to full Amazon dataset with distributed processing\n",
        "\n",
        "##  References\n",
        "- [Chen et al., 2020] Multimodal Recommender Systems\n",
        "- [McAuley et al., 2015] Image-based Recommendations on Styles and Substitutes\n",
        "- Hugging Face Transformers: https://huggingface.co/transformers/\n",
        "- TensorFlow & Keras Documentation\n",
        "- https://nijianmo.github.io/amazon/index.html\n",
        "- ChatGPT\n",
        "\n",
        "##  Code & Visualizations\n",
        "You can run the entire experiment using the notebook provided in this repository:\n",
        "- **Downloads data** from Google Drive\n",
        "- **Extracts features** using DistilBERT + ResNet50\n",
        "- **Trains classifier** with class balancing and early stopping\n",
        "- **Visualizes** training curves and confusion matrix\n",
        "\n",
        "The model is generalizable and can be adapted to any multimodal recommendation task."
      ],
      "id": "0JqByBCJT8Al"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkjs8PFUT8An"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (uncomment if needed)\n",
        "# !pip install tensorflow numpy pandas scikit-learn matplotlib pillow tqdm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "id": "mkjs8PFUT8An"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMoEf40bT8An"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install gdown to download from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "# STEP 2: Download files using their IDs\n",
        "import gdown\n",
        "\n",
        "# File 1: Amazon_Fashion.jsonl\n",
        "review_file_id = \"1s3H_MI9GXw6TZ4cnQBNu2nzYuqYgIyox\"\n",
        "meta_file_id = \"1KYzuJznVDbZeAigQKlDzNFm-JNBkDdQI\"\n",
        "\n",
        "# Output paths\n",
        "review_path = \"Amazon_Fashion.jsonl\"\n",
        "meta_path = \"meta_Amazon_Fashion.jsonl\"\n",
        "\n",
        "# Download from Google Drive\n",
        "gdown.download(id=review_file_id, output=review_path, quiet=False)\n",
        "gdown.download(id=meta_file_id, output=meta_path, quiet=False)\n",
        "\n",
        "# STEP 3: Read limited number of lines from .jsonl files\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def load_jsonl_clean(path, max_lines=25000):\n",
        "    records = []\n",
        "    with open(path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            try:\n",
        "                records.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping line {i}: {e}\")\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Load only first 25000 lines from each file\n",
        "reviews_df = load_jsonl_clean(review_path, max_lines=25000)\n",
        "meta_df = load_jsonl_clean(meta_path, max_lines=25000)\n",
        "\n",
        "# Preview structure\n",
        "print(\"‚úÖ Reviews columns:\", reviews_df.columns)\n",
        "print(\"‚úÖ Metadata columns:\", meta_df.columns)\n"
      ],
      "id": "jMoEf40bT8An"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep relevant fields\n",
        "reviews_df = reviews_df[['parent_asin', 'text', 'rating']].dropna()\n",
        "meta_df = meta_df[['parent_asin', 'images']].dropna()\n",
        "\n",
        "# Only keep entries with at least one image\n",
        "meta_df = meta_df[meta_df['images'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
        "\n",
        "# Merge on ASIN\n",
        "merged_df = pd.merge(reviews_df, meta_df, on='parent_asin')\n",
        "\n",
        "# Extract image URL (try hi_res, else thumb or large)\n",
        "def extract_first_image_url(image_list):\n",
        "    if isinstance(image_list, list) and len(image_list) > 0:\n",
        "        img_entry = image_list[0]\n",
        "        for key in ['hi_res', 'large', 'thumb']:\n",
        "            if key in img_entry and img_entry[key]:\n",
        "                return img_entry[key]\n",
        "    return None\n",
        "\n",
        "merged_df['image_url'] = merged_df['images'].apply(extract_first_image_url)\n",
        "merged_df = merged_df.dropna(subset=['image_url'])\n",
        "\n",
        "# Final clean columns\n",
        "merged_df = merged_df[['parent_asin', 'text', 'rating', 'image_url']]\n",
        "merged_df.rename(columns={'text': 'reviewText', 'rating': 'overall'}, inplace=True)\n",
        "\n",
        "print(\"Final merged dataset shape:\", merged_df.shape)\n",
        "merged_df.head()\n"
      ],
      "metadata": {
        "id": "ZERwO9rlmqz2"
      },
      "id": "ZERwO9rlmqz2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directory to save images\n",
        "image_dir = 'product_images'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Limit to first 5000 entries with non-null image_url\n",
        "subset_df = merged_df.dropna(subset=['image_url']).head(2000).copy()\n",
        "\n",
        "\n",
        "# Function to download and save an image\n",
        "def download_image(url, image_id, save_dir):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        save_path = os.path.join(save_dir, f'{image_id}.jpg')\n",
        "        img.save(save_path)\n",
        "        return save_path\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Download images and track paths\n",
        "local_paths = []\n",
        "\n",
        "for idx, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"üì• Downloading images (limit 2000)\"):\n",
        "    url = row['image_url']\n",
        "    image_id = row['parent_asin']  # Or 'asin' if that's what you're using\n",
        "    save_path = download_image(url, image_id, image_dir)\n",
        "    local_paths.append(save_path)\n",
        "\n",
        "# Attach image paths and clean up\n",
        "subset_df['image_path'] = local_paths\n",
        "subset_df = subset_df.dropna(subset=['image_path']).reset_index(drop=True)\n",
        "\n",
        "print(\"‚úÖ Images successfully downloaded:\", len(subset_df))\n",
        "\n",
        "# You can now use `subset_df` for feature extraction and training\n"
      ],
      "metadata": {
        "id": "rFpAOJzKulU0"
      },
      "id": "rFpAOJzKulU0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your HF token here\n",
        "token = os.environ.get(\"HF_TOKEN\")\n",
        "login(token)\n",
        "\n",
        "\n",
        "# Make sure 'reviewText', 'parent_asin', 'overall' are in merged_df\n",
        "merged_df['image_path'] = merged_df['parent_asin'].apply(lambda x: os.path.join('product_images', f'{x}.jpg'))\n",
        "merged_df = merged_df[merged_df['image_path'].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "# ------------------------------\n",
        "# 1. DistilBERT for Text Features\n",
        "# ------------------------------\n",
        "print(\"üî§ Extracting DistilBERT embeddings...\")\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "bert_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='tf', padding='max_length', truncation=True, max_length=128)\n",
        "    outputs = bert_model(inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "    return cls_embedding.numpy().flatten()\n",
        "\n",
        "text_features = np.array([get_bert_embedding(text) for text in tqdm(merged_df['reviewText'].fillna(\"\"))])\n",
        "np.save(\"text_features_bert.npy\", text_features)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. ResNet50 for Image Features\n",
        "# ------------------------------\n",
        "print(\"üñºÔ∏è Extracting ResNet50 image features...\")\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "def extract_image_feature(img_path):\n",
        "    try:\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        features = resnet.predict(x, verbose=0)\n",
        "        return features.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error processing {img_path}: {e}\")\n",
        "        return np.zeros((2048,))\n",
        "\n",
        "image_features = np.array([extract_image_feature(path) for path in tqdm(merged_df['image_path'])])\n",
        "np.save(\"image_features_resnet.npy\", image_features)\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Combine Features + Labels\n",
        "# ------------------------------\n",
        "print(\"üîó Combining features...\")\n",
        "\n",
        "combined_features = np.hstack((text_features, image_features))\n",
        "ratings = merged_df['overall'].values\n",
        "\n",
        "print(\"Combined features shape:\", combined_features.shape)\n",
        "print(\"Ratings shape:\", ratings.shape)\n",
        "\n",
        "np.save(\"combined_features_bert_resnet.npy\", combined_features)\n",
        "np.save(\"ratings.npy\", ratings)\n",
        "\n",
        "print(\"‚úÖ Feature extraction complete using BERT + ResNet50.\")\n"
      ],
      "metadata": {
        "id": "R5YvmBWCvXy9"
      },
      "id": "R5YvmBWCvXy9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Features and Labels\n",
        "# -------------------------------\n",
        "X = np.load(\"combined_features_bert_resnet.npy\")\n",
        "y = np.load(\"ratings.npy\")\n",
        "\n",
        "# Ensure labels are within [1, 5] and integers\n",
        "y = np.clip(np.round(y), 1, 5).astype(int)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Train-Test Split & One-Hot Labels\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "num_classes = 5\n",
        "y_train_cat = to_categorical(y_train - 1, num_classes)\n",
        "y_test_cat = to_categorical(y_test - 1, num_classes)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Compute Class Weights\n",
        "# -------------------------------\n",
        "cw = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(cw))\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Build the Classification Model\n",
        "# -------------------------------\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X.shape[1],)),\n",
        "    tf.keras.layers.Dense(1024, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Optimizer and Compilation\n",
        "# -------------------------------\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Callbacks\n",
        "# -------------------------------\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=12,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Train the Model\n",
        "# -------------------------------\n",
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    validation_data=(X_test, y_test_cat),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Plot Training Curves\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title(\"Loss Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.title(\"Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mkzLJIKuOsJB"
      },
      "id": "mkzLJIKuOsJB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict classes\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1) + 1  # +1 to match rating scale\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n‚úÖ Test Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[1,2,3,4,5])\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Actual vs Predicted Plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([1, 5], [1, 5], 'r--', label=\"Perfect Match\")\n",
        "plt.xlabel(\"Actual Ratings\")\n",
        "plt.ylabel(\"Predicted Ratings\")\n",
        "plt.title(\"Actual vs Predicted Ratings\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dng3xyxwO3b7"
      },
      "id": "dng3xyxwO3b7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}